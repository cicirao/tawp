<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Practical Impact - a thesis</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" media="screen,print" href="css/reset.css" />
    <script src="css/modernizr.custom.99475.js"></script>
    <link rel="stylesheet" href="css/screen.css" />
    <link rel="icon" href="css/gfx/favicon.ico" type="image/x-icon" />
  </head>
  <body>
    <main>
    <h1>
      Practical impact
    </h1>
      <article id="what-is-practical-impact">
      <h2>
        What is “practical impact”?  It's when research informs practice.
      </h2>
        <div class="lightbox">
          <p>
            Practitioners do things.  Researchers study how and why practitioners do the things they do.  Ideally, when researchers identify some previously unknown correlation between what practitioners do and the results they achieve, the practitioners can use that knowledge to improve their ability to get the result they want.
          </p>
          <p>
            This conversion of knowledge gained by researchers into a tool useable by practitioners is what I'm calling “practical impact”.
          </p>
        </div>
      </article>
      <article id="why-is-practical-impact-interesting">
      <h2>
        Why is the “practical impact” of research interesting?
      </h2>
        <section>
        <h3>
          On-going empirical research into trivial correlations in social sciences
        </h3>
          <p>
            As evidenced by Schwab et al. (2011), in the social sciences it is often the case that merely demonstrating that an empirical study achieves statistical significant is considered the same as demonstrating that the study represents knowledge important enough to warrant being published.
          </p>
          <p>
            However, these days very large data sets are easy to find and even easier to analyze (when compared to earlier parts of the twentieth century, which is when most empirical methods were being developed and popularized), and such data have an inherently massive statistical power.  Meaning that they make it possible to separate even the tiniest of correlations between elements in that data from the statistical “noise” of synchronous yet random variations in the same data.
          </p>
          <p>
            Such inevitable correlations, while statistically significant, are often trivial when judged by any other measure.  [Schwab talks about…]
          </p>
          <p>
            What I propose is to judge these inevitable correlations by whether or not they become a tool for use by practitioners.  Especially in the social sciences, where controlled experiments are difficult if not impossible, the best confirmation of both the existence and the importance of a phenomenon is when the knowledge about that phenomenon is taken up by practitioners.
          <p>
        </section>
        <section>
        <h3>
          Lack of relevance in management research
        </h3>
          <p>
            <!--[need to go mining in my Org Comm. paper]-->
          </p>
        </section>
        <section>
        <h3>
          Increasing demand to demonstrate practical value of research by funding agencies
        </h3>
          <p>
            The HEFCE is intending to use the value of research to practitioners as part of its decision process for what research it will recommend gets ongoing government funding.  Other European institutions are doing similar things [pull refs from HEFCE-REF overview paper].
          </p>
        </section>
        <section>
        <h3>
          Value of practical impact may not be captured by citation metrics
        </h3>
          <p>
            As funding agencies begin to track and use information of the practical value of research, research institutions themselves will need to start keeping an eye on the same thing.  Currently in academia, research is most often judged using various bibliometrics, all of which are tied to citations in academic journals.  But citation-based informetrics are almost entirely isolated to academia, and gather no information from practice or practitioners.
          </p>
          <p>
            Which means that “practical impact” is not being directly captured by citation-based metrics (<a href="references#mohammadi2014">Mohammadi <i>et al.</i> 2014</a>).  Over the long term, adoption of a given piece of research knowledge will eventually circle back around and increase the citation metrics of the orignal publication, through the citations of researchers working on new studies who observe the knowledge being used in practice, and reference it.
          </p>
          <p>
            But that cycle will be slow and lossy, as it is dependent on researchers not only recognizing the use of knowledge from previous research, but also recognizing where it came from and citing that source when they publish their own work.
          </p>
          <p>
            So, citation-based metrics might reflect “practical impact”, but they do not capture it in a timely or accurate manner.
          </p>
        </section>
      </article>
      <article id="tracking-practical-impact">
      <h2>
        Tracking practical impact
      </h2>
        <section>
        <h3>
        Altmetrics breaks out of academic silo that isolates most bibliometric methods
        </h3>
          <p>
            “Altmetrics” can refer to either the specific research-metric product from Altmetrics, LLC, or to any metric which uses data from outside of the databases of academic journals used by most, if not all, citation-based metrics.
          </p>
          <p>
            Both of those sorts of “altmetrics” have the potential to capture some of the practical impact of research, as they are not designed to exclusively use data from academia.  But they also don't exclude data from academia, either, which can obscure evidence of practical impact with the social chatter of academicians.
          </p>
          <p>
            Similarly, while methods for tracking the impact of research that use data passively provided by both researchers and practitioners (e.g., readership analysis), break out of the academia-only restriction that is inherent with a reliance on rigorous citations for evidence of impact, they also dilute the impact on practitioners in the flood of data from academics.
          </p>
          <p>
            An example is the study by Mohammadi et al. (2014) on Mendeley readership data.  As the authors explain, “Although the Mendeley API provides information related to the discipline, academic status and country of readers for each record, it only reports percentages rather than raw data and only gives information about the top three categories.”  So, if one of the top three categories of readers isn't a practitioner category, almost no information about an articles impact on practice is available.
          </p>
        </section>
        <section>
        <h3>
        The Centre for Science and Technology Studies (CWTS)
        </h3>
          <p>
            CWTS research branch looking for “societal value of research”
          </p>
        </section>
        <section>
        <h3>
          HEFCE-REF
        </h3>
          <p>
            HEFCE-REF – requires submission of “cases” demonstrating impact
          </p>
          <p>
            Review board for submitted cases comprises both researchers and practitioners
          </p>
        </section>
      </article>
      <article id="uses-of-practical-impact-information">
      <h2>
        Uses of practical impact information
      </h2>
        <section>
        <h3>
          Determination of funding (e.g., HEFCE-REF)
        </h3>
        </section>
        <section>
        <h3>
          Judging “quality of research”
        </h3>
        </section>
        <section>
        <h3>
          Improving knowledge transfer between researchers and practitioners
        </h3>
        </section>
      </article>
      <article id="hazards-of-ignoring-practical-value">
      <h2>
        Hazards of ignoring practical value
      </h2>
        <section>
        <h3>
          “Gaming” system through statistical power [see: Problems]
        </h3>
        </section>
        <section>
        <h3>
          Losing funding [see: HEFCE-REF]
        </h3>
        </section>
        <section>
        <h3>
          Rewards that are based only citation-metrics begets research that maximizes citations, and only citations
        </h3>
        </section>
        <section>
        <h3>
          Is it ethical to leave practitioners ignorant of knowledge that would improve their outcomes?
        </h3>
        </section>
      </article>
      <article id="hazards-of-making-practical-value-requisite">
      <h2>
        Hazards of making practical value requisite
      </h2>
        <div class="lightbox">
          <p>
            If it is hazardous to ignore the practical impact of research, then should it not be required that all research have some practical impact?  A form of this requirement was proposed by <a href="references.html#mitroff1998">Mitroff (1998)</a>, who claimed that any research without practical impact was not <i>true</i>, and therefore shouldn't be published.  To explain why practical impact is a requirement for truth he wrote:
          <p>
            <blockquote cite="references.html#mitroff1998">
            <p>
              <a class="nocolor" href="references.html#mitroff1998">Pragmatism is the philosophical school that posits that truth is that which makes a significant difference in the lives of humans. Something&mdash;an action, empirical finding, proposition, conjecture, theorem—that is true in theory (i.e., in the abstract only) but makes no difference in the lives of humans is not a truth for pragmatism. Thus, contrary to what many academics believe, truth is not solely a property of formal propositions, theorems, research findings, and so forth but of ethical actions (i.e., actions that eliminate, or make significant headway in eliminating, some important human problem).</a>
            </p>
            </blockquote>
          <p>
            The most obvious problem with requiring practical impact for research is that it would block research into the fundamental laws and principles that underlie every field.  It would hamper research that attempted to synthesize general solutions to groups of related specific problems, since the general solution might “<q cite="references.html#hulin2001">be broad and theoretical and… difficult to apply</q>” (<a href="references.html#hulin2001">Hunlin 2001</a>).
          </p>
        </div>
      </article>
      <article id="rewarding-practical-impact">
      <h2>
        Rewarding practical impact
      </h2>
        <div class="lightbox">
          <p>
            If there are hazards to requiring that research has practical impact, but also hazards to ignoring the practical impact of research, what should be done?  Researchers should be rewarded for the practical impact of their research.  This makes practical impact important, without making it so important that it hampers the free exploration that is vital to research.
          </p>
          <p>
            Without some sort of reward for practical impact, there is little incentive for researchers to spend time encouraging or nurturing the practical impact of their work.  Without the sorts of rewards that they get for presenting at academic conferences, or writing for academic journals, researchers won't spend their time and energy presenting at practitioner conferences, or writing for practitioner journals (<a href="references.html#latham2007">Latham 2007</a>).
          </p>
        </div>
      </article>
      <article id="categorizing-practical-impact">
      <h2>
      Categorizing practical impact
      </h2>
        <section>
        <h3>
          Comparisons should always be within the same, or similar, fields
        </h3>
          <p>
            One of the more common criticisms of the Journal Impact Factor is that “<q cite="references.html#moed2010">it is improper to make comparisons between citation counts generated in different research fields, because the ‘citation potential’ can vary significantly from one field to another</q>” (<a href="references.html#moed2010">Moed 2010</a>).
          </p>
          <p>
            Similar to “citation potential”, the potential for practical impact varies between fields than does the “citation potential”.  <a href="references.html#schwab2011">Schwab <i>et al.</i> (2011)</a> imply this disparity when they discuss why the social sciences continue to embrace null-hypothesis significant testing, while medical research has moved to replace it with methodologies that are, among other things, more predictive of practical impact.  The shift was drive by the fact that “Medical research… makes more of a difference to more people, and draws much more attention. Thus, medical researchers have greater incentive to measure and document effects of their work…”.  In other words, medical research has a much greater potential for practical impact than the social sciences.
          </p>
          <p>
            So, it would not be appropriate, for example, to measure the practical impact of research in the social science against a bar set by medical research.
          </p>
        </section>
        <section>
        <h3>
          Qualitative process
        </h3>
          <div class="subcontainer">
          <h4>
            Too many possible sources for practical impact data to equitably quantify
          </h4>
            <p>
              Altmetrics demonstrates one possible way to track practical impact.  It looks for (loosely defined) citations in social media data streams as a way to supplement citation-based metrics.  If the actors involved in those social media can be separated into classes of “academics” and “practitioners”, then the references to research made by members of the  practitioner class can become evidence of practical impact.
            </p>
            <p>
              Another existing example is analysis of data about the readership of research. <a href="references#mohammadi2014">Mohammadi <i>et al.</i> (2014)</a> looked at the readership data of Mendeley, where users self-identify their profession, so that the Mendely API can distinguish between academic and non-academic readers.  Being read by non-academic users can be evidence of the practical impact of a research article.  Though, there are some problems with trying to use the Mendeley API this way, since “<q cite="references#mohammadi2014">the Mendeley API provides information related to the discipline, academic status and country of readers for each record, [but] it only reports percentages rather than raw data and only gives information about the top three categories</q>” (<a href="references#mohammadi2014"><i>ibid</i></a>).
            </p>
            <p>
              But, following the logic of Mendeley-readership analysis, any computer-mediated reading process becomes a potential path for identifying practical impact.
            </p>
            <p>
              However, not all of these possible paths are going to present the same strength of evidence of practical impact.  The nature of the evidence&mdash;e.g., the reading of an article as opposed to actively tweeting about it&mdash;will effect its strength, as will the particular path from which it comes&mdash;e.g., readership data from a source that only reports the top-three readership classes versus one that reports the top ten.
            </p>
            <p>
              So doing quantitative transformations and comparisons with the evidence of practical impact being proposed here is simply not reasonable, or defensible.
            </p>
          </div>
          <div class="subcontainer">
          <h4>
            Different fields have varying levels of practical impact
          </h4>
            <p>
              Another impediment to the quantification of practical impact evidence is that different fields will have different levels of practical impact in general.  So, a research paper that has an important level of practical impact in one field would only qualify as having an apparent level of practical impact in another.  Researchers working in more esoteric fields should not be measured against bars set by researchers in fields like medical research which are very closely tied to practice.
            </p>
          </div>
        </section>
        <section>
        <h3>
          Levels
        </h3>
          <div class="subcontainer">
          <h4>
            U – Unknown/undefined
          </h4>
            <p>
              No examples of practical impact
            </p>
          </div>
          <div class="subcontainer">
          <h4>
            A – Apparent
          </h4>
            <p>
              At least one example of practical impact
            </p>
          </div>
          <div class="subcontainer">
          <h4>
            I – Important
          </h4>
            <p>
              Multiple examples of practical impact; has more impact than other research in the same field (with at least an A-level)
            </p>
          </div>
        </section>
      </article>
      <article id="conclusion-and-reiteration">
      <h2>
        Conclusion / Reiteration
      </h2>
        <div class="lightbox">
          <p>
            Bibliometric methods are well established and widely used, but they reinforce the isolation of academic research from real-world practice.  That isolation has a number of consequences that include: the proliferation of trivial empirical studies; the failure of practitioners to benefit from the knowledge gains of researchers; and the increasingly skeptical scrutiny of the public funding of research.
          </p>
          <p>
            While requiring practical impact of academic research is both unlikely to be adopted and hazardous to the ongoing advancement of knowledge, the practical impact of research must be rewarded before researchers can be expected to look for and exploit opportunities for achieving practical impact in their research.
          </p>
          <p>
            Effective reward systems will require effective means of tracking the practical impact of research.  Current bibliometric and altmetric methodologies fail to capture practical impact in anything more than a glancing manner.
          </p>
          <p>
            An effective practical impact metric could draw data from a number of paths, including social media and readership information.  It would rank the practical impact of research with three non-quantitative levels: Unknown, Apparent and Important.  The distinction between the “A” and “I” levels would be based on comparisons within a field, to avoid disparities between fields that have inherently different potentials for practical impact, and established behaviors that might skew the tracking of practical impact.
          </p>
        </div>
      </article>
    </main>
    <nav>
      <div>
        <a href="index.html">
          Abstract/Index
        </a>
      </div>
      <div>
        <a href="practical-impact-thesis.html">
          Thesis
        </a>
      </div>
      <div>
        <a href="references.html">
          References
        </a>
      </div>
    </nav>
    <footer>
    </footer>
  </body>
</html>
